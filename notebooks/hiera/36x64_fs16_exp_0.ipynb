{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f01514e-4902-4c4f-9d48-5adc9726f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wq.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4497c5f4-9d00-4bd5-a391-e492a3ecb9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1646., 1602., 1691., 1626., 1628., 1665., 1609., 1687., 1622.,\n",
       "        1608.]),\n",
       " array([-8.83537233e-02, -7.06801862e-02, -5.30066416e-02, -3.53331007e-02,\n",
       "        -1.76595598e-02,  1.39772892e-05,  1.76875219e-02,  3.53610665e-02,\n",
       "         5.30346036e-02,  7.07081407e-02,  8.83816853e-02]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.wq.weight.data.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2399d3e0-2669-489d-889f-f644650c793b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wq.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637b327b-4b4d-4ed9-9918-818e5825c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from experanto.datasets import ChunkDataset, SimpleChunkedDataset\n",
    "from experanto.utils import LongCycler, MultiEpochsDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ecf779-64e2-4c53-930c-4bc96109fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional packages\n",
    "# pip install hiera-transformer\n",
    "# pip install -U pytorch_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f5a3f-8720-4222-a63c-b3605b61c5e7",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8aa514-908e-4e00-af7a-06830084b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_size = [36, 64]\n",
    "screen_chunk_size = 16\n",
    "chunk_size = 16\n",
    "\n",
    "dim_head = 128\n",
    "num_heads = 1\n",
    "drop_path_rate = 0\n",
    "mlp_ratio=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a43689-cdb6-4443-a57d-0a227fb65d45",
   "metadata": {},
   "source": [
    "### get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72edb2a0-b392-40be-9e1a-054aacb4593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.dataloaders import get_multisession_dataloader\n",
    "\n",
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "paths = ['dynamic29513-3-5-Video-full',\n",
    "         'dynamic29514-2-9-Video-full',\n",
    "         'dynamic29755-2-8-Video-full',\n",
    "         'dynamic29647-19-8-Video-full',\n",
    "         'dynamic29156-11-10-Video-full',\n",
    "         'dynamic29623-4-9-Video-full',\n",
    "         'dynamic29515-10-12-Video-full',\n",
    "         'dynamic29234-6-9-Video-full',\n",
    "         'dynamic29712-5-9-Video-full',\n",
    "         'dynamic29228-2-10-Video-full'\n",
    "        ]\n",
    "full_paths = [path.join(\"/data/mouse_polly/\", f) for f in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f1c64c-31a4-4391-85ef-6f35d7085c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cfg.dataset.global_chunk_size = None\n",
    "cfg.dataset.global_sampling_rate = None\n",
    "\n",
    "cfg.dataset.modality_config.screen.chunk_size = screen_chunk_size\n",
    "cfg.dataset.modality_config.screen.sampling_rate = 8\n",
    "cfg.dataset.modality_config.responses.chunk_size = chunk_size\n",
    "cfg.dataset.modality_config.responses.sampling_rate = 8\n",
    "cfg.dataset.modality_config.eye_tracker.chunk_size = 8\n",
    "cfg.dataset.modality_config.eye_tracker.sampling_rate = 8\n",
    "cfg.dataset.modality_config.treadmill.chunk_size = 8\n",
    "cfg.dataset.modality_config.treadmill.sampling_rate = 8\n",
    "\n",
    "cfg.dataset.modality_config.screen.sample_stride = 1\n",
    "cfg.dataset.modality_config.screen.include_blanks=True\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"train\"}\n",
    "cfg.dataset.modality_config.screen.transforms.Resize.size = video_size\n",
    "\n",
    "cfg.dataloader.num_workers=1\n",
    "cfg.dataloader.prefetch_factor=1\n",
    "cfg.dataloader.batch_size=16\n",
    "cfg.dataloader.pin_memory=True\n",
    "cfg.dataloader.shuffle=True\n",
    "\n",
    "train_dl = get_multisession_dataloader(full_paths, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14b18d-346d-48c4-b12e-e7b8e0f28820",
   "metadata": {},
   "source": [
    "### get Hiera backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc0b8f9-b374-479f-b166-801bf9d22851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 9, 16, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install hiera-transformer\n",
    "from hiera import Hiera\n",
    "tiny_hiera = Hiera(input_size=(screen_chunk_size, video_size[0], video_size[1]),\n",
    "                     num_heads=1,\n",
    "                     embed_dim=64,\n",
    "                     stages=(1, 1,), # 4 transformer layers \n",
    "                     q_pool=1, \n",
    "                     in_chans=1,\n",
    "                     q_stride=(1, 1, 1,),\n",
    "                     mask_unit_size=(1, 8, 8),\n",
    "                     patch_kernel=(3, 9, 9),\n",
    "                     patch_stride=(2, 4, 4),\n",
    "                     patch_padding=(1, 3, 3),\n",
    "                     sep_pos_embed=True,\n",
    "                     drop_path_rate=drop_path_rate,\n",
    "                     mlp_ratio=4,)\n",
    "\n",
    "tiny_hiera = tiny_hiera.cuda().to(torch.float32);\n",
    "example_input = torch.ones(8,1,screen_chunk_size, 36,64).to(\"cuda\", torch.float32)\n",
    "out = tiny_hiera(example_input, return_intermediates=True);\n",
    "hiera_output = out[-1][-1]\n",
    "hiera_output.shape # (b, t, h, w, c): (8, 4, 9, 16, 192)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e892e-0e01-4fda-bec9-13179f415692",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbce1ed2-9c69-43e0-91c9-a35fc2e4b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MouseHieraBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 dls,\n",
    "                 chunk_size,\n",
    "                 dim=192,\n",
    "                 dim_head=32,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=4,):\n",
    "        super().__init__()\n",
    "        self.backbone=backbone\n",
    "        self.num_heads=num_heads\n",
    "        self.dim_head=dim_head\n",
    "        self.dim_q = dim_head*num_heads\n",
    "        self.wq = nn.Linear(self.dim_q, self.dim_q, bias=False)\n",
    "        self.wk = nn.Linear(dim, self.dim_q, bias=False)\n",
    "        self.wv = nn.Linear(dim, self.dim_q, bias=False)\n",
    "        self.wo = nn.Linear(self.dim_q, self.dim_q, bias=False)\n",
    "        \n",
    "        self.neuron_proj = nn.Linear(self.dim_q, chunk_size, bias=False)\n",
    "        \n",
    "        \n",
    "        self.kv_norm=torch.nn.RMSNorm(dim)\n",
    "        self.q_norm=torch.nn.RMSNorm(self.dim_q)\n",
    "        self.qkv_norm=torch.nn.RMSNorm(self.dim_q)\n",
    "        self.mlp = MLP(dim=self.dim_q, hidden_dim=int(self.dim_q * mlp_ratio))\n",
    "        self.readout = nn.ModuleDict()\n",
    "        self.activation = nn.Softplus(beta=0.1) # probably a much better activation than ELU+1\n",
    "        for k, v in dls.loaders.items():\n",
    "            n_neurons = next(iter(v))[\"responses\"].shape[-1]\n",
    "            self.readout[k] = IndexedLinearReadout(n_neurons, \n",
    "                                                   in_features=dim_head*num_heads,\n",
    "                                                   dim_head=dim_head, \n",
    "                                                   num_heads=num_heads, \n",
    "                                                  )\n",
    "            \n",
    "    def forward(self, x, key):\n",
    "        x = self.backbone(x, return_intermediates=True)[-1][-1]\n",
    "        b, t, h, w, d = x.shape\n",
    "        x = self.kv_norm(x)\n",
    "        x = x.view(b, -1, d) # (B, t*h*w, D)\n",
    "        k, v = self.wk(x), self.wv(x)\n",
    "        q = self.q_norm(self.readout[key].query)\n",
    "        n = q.shape[1] # number of neurons\n",
    "        q = q.repeat(b, 1, 1) # repeat query for number of batches\n",
    "        q_attn = self.wq(q)\n",
    "        q_attn = q.view(b, -1, self.num_heads, self.dim_head).transpose(1, 2)\n",
    "        k = k.view(b, -1, self.num_heads, self.dim_head).transpose(1, 2) # (B, H, S, D)\n",
    "        v = v.view(b, -1, self.num_heads, self.dim_head).transpose(1, 2) # (B, H, S, D)\n",
    "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "            o = F.scaled_dot_product_attention(q_attn, k, v)\n",
    "        # (B, H, S, D) -> (B, N, D), with N = num_neurons\n",
    "        o = o.transpose(1,2).contiguous().view(b, -1, self.dim_q)\n",
    "        o = self.wo(o) + q\n",
    "        o = self.qkv_norm(o)  \n",
    "        o = self.mlp(o) + o\n",
    "        o = self.neuron_proj(o) # (B, N, D) -> (B, N, t)\n",
    "        o = o + self.readout[key].bias\n",
    "        o = self.activation(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adacf91a-d54e-4e4d-92d2-72ec5c2b89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "    def init_weights(self, std=.5, cutoff_factor: int = 3):\n",
    "        \"\"\"See `TorchTitan <https://github.com/pytorch/torchtitan/blob/40a10263c5b3468ffa53b3ac98d80c9267d68155/torchtitan/models/llama/model.py#L403>`__.\"\"\"\n",
    "        nn.init.trunc_normal_(\n",
    "            self.net[0].weight,\n",
    "            mean=0.0,\n",
    "            std=std,\n",
    "            a=-cutoff_factor * std,\n",
    "            b=cutoff_factor * std,\n",
    "        )\n",
    "        nn.init.trunc_normal_(\n",
    "            self.net[2].weight,\n",
    "            mean=0.0,\n",
    "            std=std,\n",
    "            a=-cutoff_factor * std,\n",
    "            b=cutoff_factor * std,\n",
    "        )\n",
    "        self.net[0].bias.data.zero_()\n",
    "        self.net[2].bias.data.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07a6270-174c-42d7-8f19-2591ca4ddb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedLinearReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    Readout module for MTM models with selectable weights based on \n",
    "    input IDs. Based on :class:`torch.nn.Linear`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_ids: int,\n",
    "        in_features: int = 384,\n",
    "        dim_head=32,\n",
    "        num_heads=4,\n",
    "        bias: bool = True,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float32,\n",
    "        init_std: float = 0.02,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        self.unique_ids = unique_ids\n",
    "        self.in_features = in_features\n",
    "        self.init_std = init_std\n",
    "        self.query = nn.Parameter(\n",
    "            torch.empty(1, unique_ids, dim_head*num_heads, **factory_kwargs)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.empty(1, unique_ids, 1, **factory_kwargs)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self, cutoff_factor: int = 3):\n",
    "        \"\"\"See `TorchTitan <https://github.com/pytorch/torchtitan/blob/40a10263c5b3468ffa53b3ac98d80c9267d68155/torchtitan/models/llama/model.py#L403>`__.\"\"\"\n",
    "        readout_std = self.in_features**-0.5\n",
    "        nn.init.trunc_normal_(\n",
    "            self.query,\n",
    "            mean=0.0,\n",
    "            std=readout_std,\n",
    "            a=-cutoff_factor * readout_std,\n",
    "            b=cutoff_factor * readout_std,\n",
    "        )\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddde36-dfd5-44b3-8961-b67d970422a5",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "512db87e-b274-4208-9579-0c36d322d213",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/data/experanto/experanto/utils.py:105\u001b[0m, in \u001b[0;36mMultiEpochsDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1438\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_workers()\n\u001b[0;32m-> 1438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m \n\u001b[1;32m   1442\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m backbone_dim \u001b[38;5;241m=\u001b[39m hiera_output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMouseHieraBase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtiny_hiera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdim_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mMouseHieraBase.__init__\u001b[0;34m(self, backbone, dls, chunk_size, dim, dim_head, num_heads, mlp_ratio)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftplus(beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# probably a much better activation than ELU+1\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dls\u001b[38;5;241m.\u001b[39mloaders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 30\u001b[0m     n_neurons \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout[k] \u001b[38;5;241m=\u001b[39m IndexedLinearReadout(n_neurons, \n\u001b[1;32m     32\u001b[0m                                            in_features\u001b[38;5;241m=\u001b[39mdim_head\u001b[38;5;241m*\u001b[39mnum_heads,\n\u001b[1;32m     33\u001b[0m                                            dim_head\u001b[38;5;241m=\u001b[39mdim_head, \n\u001b[1;32m     34\u001b[0m                                            num_heads\u001b[38;5;241m=\u001b[39mnum_heads, \n\u001b[1;32m     35\u001b[0m                                           )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "backbone_dim = hiera_output[-1][-1].shape[-1]\n",
    "model = MouseHieraBase(backbone=tiny_hiera, \n",
    "                        dls=train_dl, \n",
    "                        chunk_size=chunk_size,\n",
    "                        dim=backbone_dim, \n",
    "                        dim_head=dim_head,\n",
    "                        num_heads=num_heads,\n",
    "                       mlp_ratio=mlp_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c370d-db23-4fa6-8cb6-a34f3d72ae6a",
   "metadata": {},
   "source": [
    "### performance boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bb7bef-0c85-4b4a-b072-dba2e947617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 32\n",
    "model = model.cuda().to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db1feb-d92b-426b-925e-5ae114465d83",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce03e89e-23d5-4cbe-b96e-8ecb2cee2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pytorch_warmup\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "lr = 3e-4\n",
    "T_max = 5e5\n",
    "eta_min = 5e-6\n",
    "\n",
    "criteria = nn.PoissonNLLLoss(log_input=False, reduction='mean')\n",
    "opt = Adam(model.parameters(), lr=lr, )\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt,\n",
    "                                                          T_max=T_max, \n",
    "                                                          eta_min=eta_min)\n",
    "\n",
    "warmup_scheduler = warmup.UntunedLinearWarmup(opt)\n",
    "\n",
    "from torch import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628553b7-f11f-43e6-a5f9-715c13505ee7",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5515d2-3d42-4e90-b100-26b296934033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "import hydra\n",
    "from hydra import utils\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://at-compute015:5000/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8fe4b1d-536b-4ba4-aa25-2046211585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mousehiera-base-0\"\n",
    "current_experiment=dict(mlflow.get_experiment_by_name(experiment_name))\n",
    "experiment_id=current_experiment['experiment_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae39a6-7b18-4450-a528-f060d0bbc87d",
   "metadata": {},
   "source": [
    "# train messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f31bcf-dab8-45d1-82d3-fc16c723da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "cfg.dataset.global_sampling_rate = None\n",
    "cfg.dataset.global_chunk_size = None\n",
    "cfg.dataset.modality_config.screen.chunk_size = 16\n",
    "cfg.dataset.modality_config.screen.sampling_rate = 8\n",
    "cfg.dataset.modality_config.responses.chunk_size = 16\n",
    "cfg.dataset.modality_config.responses.sampling_rate = 8\n",
    "cfg.dataset.modality_config.eye_tracker.chunk_size = 8\n",
    "cfg.dataset.modality_config.eye_tracker.sampling_rate = 8\n",
    "cfg.dataset.modality_config.treadmill.chunk_size = 8\n",
    "cfg.dataset.modality_config.treadmill.sampling_rate = 8\n",
    "cfg.dataset.modality_config.screen.include_blanks=False\n",
    "cfg.dataset.modality_config.screen.sample_stride=8\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"oracle\"}\n",
    "cfg.dataset.modality_config.screen.transforms.Resize.size = video_size\n",
    "\n",
    "\n",
    "cfg.dataloader.num_workers=1\n",
    "cfg.dataloader.prefetch_factor=1\n",
    "cfg.dataloader.batch_size=128\n",
    "cfg.dataloader.pin_memory=True\n",
    "# the multiple dataloader is an iterator that returns a tuple of (key, batch)\n",
    "val_dl = get_multisession_dataloader(full_paths[3:6], cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7410a49b-aa7f-4251-8e7f-1d68b88e1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step_():\n",
    "    targets, predictions = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i, (k, b) in tqdm(enumerate(val_dl)):\n",
    "            videos = b[\"screen\"].to(\"cuda\", torch.float32, non_blocking=True).permute(0,2,1,3,4)\n",
    "            responses = b[\"responses\"].to(\"cuda\", torch.float32, non_blocking=True)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                out = model(videos, k);\n",
    "            predictions.append(out.transpose(1,2).flatten())\n",
    "            targets.append(responses.flatten())\n",
    "        c = np.corrcoef(torch.hstack(predictions).cpu().numpy(), torch.hstack(targets).cpu().numpy(), )\n",
    "    return c[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82f1ad-b724-49d6-99c8-7c01d884bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:05,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.00014192490224909993)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16220it [08:10, 33.10it/s]\n",
      "9it [00:03,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.00014192490224909993), np.float64(0.07751743199534641)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16220it [09:25, 28.68it/s]\n",
      "9it [00:05,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.00014192490224909993), np.float64(0.07751743199534641), np.float64(0.09738803509913543)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12543it [10:46, 21.56it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "14006it [12:31, 19.21it/s]"
     ]
    }
   ],
   "source": [
    "patience = 0\n",
    "max_objective = 0\n",
    "losses, corrs, lrs, val_corrs = [], [], [], []\n",
    "with mlflow.start_run(\n",
    "    run_name=\"hiera16_36x64_fs16_base\",\n",
    "    experiment_id=experiment_id,\n",
    "    tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
    "    description=\"parent\",):\n",
    "    \n",
    "    for train_loop in range(1000):\n",
    "        model.eval();\n",
    "        current_objective = val_step_()\n",
    "        mlflow.log_metric(\"val/single_trial_corr\", current_objective)\n",
    "        if train_loop > 20:\n",
    "            max_objective = np.max(np.array(val_corrs[:-1])[~np.isnan(val_corrs[:-1])])\n",
    "        if current_objective < max_objective:\n",
    "            patience += 1\n",
    "        else:\n",
    "            patience = 0\n",
    "        if patience >3:\n",
    "            break\n",
    "        val_corrs.append(current_objective)\n",
    "        print(val_corrs)\n",
    "        for i, (k, b) in tqdm(enumerate(train_dl)):\n",
    "            model.train();\n",
    "            videos = b[\"screen\"].to(\"cuda\", torch.float32, non_blocking=True).permute(0,2,1,3,4)\n",
    "            responses = b[\"responses\"].to(\"cuda\", torch.float32, non_blocking=True)\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                out = model(videos, k);\n",
    "            loss = criteria(out.transpose(1,2), responses)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            with warmup_scheduler.dampening():\n",
    "                lr_scheduler.step()\n",
    "        for k, v in train_dl.loaders.items():\n",
    "            v.dataset.shuffle_valid_screen_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70dba1d-ac5f-4c22-ba45-17af3f7fa5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a0cf1-7524-4ea1-943a-f7f1ead46701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
