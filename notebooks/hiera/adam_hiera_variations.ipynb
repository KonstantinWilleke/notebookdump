{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f8e0c7-b663-4837-9679-f791389b88fc",
   "metadata": {},
   "source": [
    "# AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc9e33d-980b-4322-b9e5-bdaf0c3a80c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(val_corrs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(val_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637b327b-4b4d-4ed9-9918-818e5825c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from experanto.datasets import ChunkDataset, SimpleChunkedDataset\n",
    "from experanto.utils import LongCycler, MultiEpochsDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ecf779-64e2-4c53-930c-4bc96109fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional packages\n",
    "# pip install hiera-transformer\n",
    "# pip install -U pytorch_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f5a3f-8720-4222-a63c-b3605b61c5e7",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8aa514-908e-4e00-af7a-06830084b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_size = [72, 128]\n",
    "chunk_size = 8\n",
    "dim_head = 64\n",
    "num_heads = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a43689-cdb6-4443-a57d-0a227fb65d45",
   "metadata": {},
   "source": [
    "### get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72edb2a0-b392-40be-9e1a-054aacb4593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.dataloaders import get_multisession_dataloader\n",
    "\n",
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "paths = ['dynamic29513-3-5-Video-full',\n",
    "         'dynamic29514-2-9-Video-full',\n",
    "         'dynamic29755-2-8-Video-full',\n",
    "         'dynamic29647-19-8-Video-full',\n",
    "         'dynamic29156-11-10-Video-full',\n",
    "         'dynamic29623-4-9-Video-full',\n",
    "         'dynamic29515-10-12-Video-full',\n",
    "         'dynamic29234-6-9-Video-full',\n",
    "         'dynamic29712-5-9-Video-full',\n",
    "         'dynamic29228-2-10-Video-full'\n",
    "        ]\n",
    "full_paths = [path.join(\"/data/mouse_polly/\", f) for f in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f1c64c-31a4-4391-85ef-6f35d7085c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cfg.dataset.global_chunk_size = 8\n",
    "cfg.dataset.global_sampling_rate = 8\n",
    "cfg.dataset.modality_config.screen.sample_stride = 8\n",
    "cfg.dataset.modality_config.screen.include_blanks=True\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"train\"}\n",
    "cfg.dataset.modality_config.screen.transforms.Resize.size = video_size\n",
    "\n",
    "cfg.dataloader.num_workers=1\n",
    "cfg.dataloader.prefetch_factor=1\n",
    "cfg.dataloader.batch_size=128\n",
    "cfg.dataloader.pin_memory=True\n",
    "cfg.dataloader.shuffle=True\n",
    "\n",
    "train_dl = get_multisession_dataloader(full_paths, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14b18d-346d-48c4-b12e-e7b8e0f28820",
   "metadata": {},
   "source": [
    "### get Hiera backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc0b8f9-b374-479f-b166-801bf9d22851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 9, 16, 192])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install hiera-transformer\n",
    "from hiera import Hiera\n",
    "tiny_hiera = Hiera(input_size=(chunk_size, video_size[0], video_size[1]),\n",
    "                     num_heads=1,\n",
    "                     embed_dim=96,\n",
    "                     stages=(2, 2,), # 4 transformer layers \n",
    "                     q_pool=1, \n",
    "                     in_chans=1,\n",
    "                     q_stride=(1, 2, 2),\n",
    "                     mask_unit_size=(1, 4, 4),\n",
    "                     patch_kernel=(3, 8, 8),\n",
    "                     patch_stride=(2, 4, 4),\n",
    "                     patch_padding=(1, 3, 3),\n",
    "                     sep_pos_embed=True,)\n",
    "\n",
    "tiny_hiera = tiny_hiera.cuda().to(torch.float32);\n",
    "example_input = torch.ones(8,1,8,72,128).to(\"cuda\", torch.float32)\n",
    "out = tiny_hiera(example_input, return_intermediates=True);\n",
    "hiera_output = out[-1][-1]\n",
    "hiera_output.shape # (b, t, h, w, c): (8, 4, 9, 16, 192)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e892e-0e01-4fda-bec9-13179f415692",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbce1ed2-9c69-43e0-91c9-a35fc2e4b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MouseHieraSmall(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 dls,\n",
    "                 chunk_size,\n",
    "                 dim=192,\n",
    "                 dim_head=32,\n",
    "                 num_heads=4):\n",
    "        super().__init__()\n",
    "        self.backbone=backbone\n",
    "        self.num_heads=num_heads\n",
    "        self.dim_head=dim_head\n",
    "        self.wk = nn.Linear(dim, dim_head * num_heads, bias=False)\n",
    "        self.wv = nn.Linear(dim, dim_head * num_heads, bias=False)\n",
    "        self.neuron_proj = nn.Linear(dim_head * num_heads, chunk_size, bias=False)\n",
    "        self.readout = nn.ModuleDict()\n",
    "        self.activation = nn.Softplus(beta=0.5) # probably a much better activation than ELU+1\n",
    "        for k, v in dls.loaders.items():\n",
    "            n_neurons = next(iter(v))[\"responses\"].shape[-1]\n",
    "            self.readout[k] = IndexedLinearReadout(n_neurons, \n",
    "                                                   in_features=dim_head*num_heads,\n",
    "                                                   dim_head=dim_head, \n",
    "                                                   num_heads=num_heads, \n",
    "                                                  )\n",
    "            \n",
    "    def forward(self, x, key):\n",
    "        x = self.backbone(x, return_intermediates=True)[1][-1]\n",
    "        b, t, h, w, d = x.shape\n",
    "        x = x.view(b, -1, d) # (B, t*h*w, D)\n",
    "        k, v = self.wk(x), self.wv(x)\n",
    "        q = self.readout[key].query\n",
    "        n = q.shape[2] # number of neurons\n",
    "        q = q.repeat(b, 1, 1, 1) # repeat query for number of batches\n",
    "        k = k.view(b, -1, self.num_heads, self.dim_head).transpose(1, 2) # (B, H, S, D)\n",
    "        v = v.view(b, -1, self.num_heads, self.dim_head).transpose(1, 2) # (B, H, S, D)\n",
    "        o = F.scaled_dot_product_attention(q, k, v)\n",
    "            \n",
    "        # (B, H, S, D) -> (B, N, D), with N = num_neurons\n",
    "        o = o.transpose(1,2).contiguous().view(b, -1, self.num_heads * self.dim_head)\n",
    "        o = self.neuron_proj(o) # (B, N, D) -> (B, N, t)\n",
    "        o = o + self.readout[key].bias\n",
    "        o = self.activation(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528566e8-29c1-46a1-b4b3-2fea3b64da88",
   "metadata": {},
   "source": [
    "# Readout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07a6270-174c-42d7-8f19-2591ca4ddb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedLinearReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    Readout module for MTM models with selectable weights based on \n",
    "    input IDs. Based on :class:`torch.nn.Linear`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_ids: int,\n",
    "        in_features: int = 384,\n",
    "        dim_head=32,\n",
    "        num_heads=4,\n",
    "        bias: bool = True,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float32,\n",
    "        init_std: float = 0.02,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        self.unique_ids = unique_ids\n",
    "        self.in_features = in_features\n",
    "        self.init_std = init_std\n",
    "        self.query = nn.Parameter(\n",
    "            torch.empty(1, num_heads, unique_ids, dim_head, **factory_kwargs)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.empty(1, unique_ids, 1, **factory_kwargs)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self, cutoff_factor: int = 3):\n",
    "        \"\"\"See `TorchTitan <https://github.com/pytorch/torchtitan/blob/40a10263c5b3468ffa53b3ac98d80c9267d68155/torchtitan/models/llama/model.py#L403>`__.\"\"\"\n",
    "        readout_std = self.in_features**-0.5\n",
    "        nn.init.trunc_normal_(\n",
    "            self.query,\n",
    "            mean=0.0,\n",
    "            std=readout_std,\n",
    "            a=-cutoff_factor * readout_std,\n",
    "            b=cutoff_factor * readout_std,\n",
    "        )\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddde36-dfd5-44b3-8961-b67d970422a5",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "512db87e-b274-4208-9579-0c36d322d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_dim = hiera_output[-1][-1].shape[-1]\n",
    "model = MouseHieraSmall(backbone=tiny_hiera, \n",
    "                        dls=train_dl, \n",
    "                        chunk_size=chunk_size,\n",
    "                        dim=backbone_dim, \n",
    "                        dim_head=dim_head,\n",
    "                        num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c370d-db23-4fa6-8cb6-a34f3d72ae6a",
   "metadata": {},
   "source": [
    "### performance boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7bb7bef-0c85-4b4a-b072-dba2e947617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "torch._dynamo.config.cache_size_limit = 32\n",
    "model = model.cuda().to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db1feb-d92b-426b-925e-5ae114465d83",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce03e89e-23d5-4cbe-b96e-8ecb2cee2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pytorch_warmup\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "lr = 8.0e-4\n",
    "weight_decay = 0\n",
    "T_max = 5e4\n",
    "eta_min = 5e-6\n",
    "\n",
    "criteria = nn.PoissonNLLLoss(log_input=False, reduction='mean')\n",
    "opt = Adam(model.parameters(), lr=lr, weight_decay=weight_decay,)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt,\n",
    "                                                          T_max=T_max, \n",
    "                                                          eta_min=eta_min)\n",
    "\n",
    "warmup_scheduler = warmup.UntunedLinearWarmup(opt)\n",
    "\n",
    "from torch import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae39a6-7b18-4450-a528-f060d0bbc87d",
   "metadata": {},
   "source": [
    "# train messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f31bcf-dab8-45d1-82d3-fc16c723da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "cfg.dataset.global_chunk_size = 8\n",
    "cfg.dataset.global_sampling_rate = 8\n",
    "cfg.dataset.modality_config.screen.include_blanks=False\n",
    "cfg.dataset.modality_config.screen.sample_stride=8\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"oracle\"}\n",
    "cfg.dataset.modality_config.screen.transforms.Resize.size = video_size\n",
    "\n",
    "\n",
    "cfg.dataloader.num_workers=1\n",
    "cfg.dataloader.prefetch_factor=1\n",
    "cfg.dataloader.batch_size=128\n",
    "cfg.dataloader.pin_memory=True\n",
    "# the multiple dataloader is an iterator that returns a tuple of (key, batch)\n",
    "val_dl = get_multisession_dataloader(full_paths[3:4], cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7410a49b-aa7f-4251-8e7f-1d68b88e1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step():\n",
    "    targets, predictions = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i, (k, b) in tqdm(enumerate(val_dl)):\n",
    "            videos = b[\"screen\"].to(\"cuda\", torch.float32, non_blocking=True).permute(0,2,1,3,4)\n",
    "            responses = b[\"responses\"].to(\"cuda\", torch.float32, non_blocking=True)\n",
    "            out = model(videos, k);\n",
    "            predictions.append(out.transpose(1,2).to(torch.float32).cpu().numpy())\n",
    "            targets.append(responses.to(torch.float32).cpu().numpy())\n",
    "    r1 = np.vstack(np.vstack(predictions))\n",
    "    r2 = np.vstack(np.vstack(targets))\n",
    "    cs = []\n",
    "    for n in range(8202):\n",
    "        c =  np.corrcoef(r1[...,n].flatten(), r2[...,n].flatten(), )[0,1]\n",
    "        cs.append(c)\n",
    "    val_corrs = np.stack(cs).mean()\n",
    "    return val_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82f1ad-b724-49d6-99c8-7c01d884bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:05,  1.33s/it]\n",
      "260it [00:51,  5.02it/s]\n",
      "4it [00:04,  1.22s/it]\n",
      "260it [00:49,  5.24it/s]\n",
      "4it [00:04,  1.18s/it]\n",
      "260it [00:49,  5.25it/s]\n",
      "4it [00:04,  1.13s/it]\n",
      "260it [00:49,  5.28it/s]\n",
      "4it [00:04,  1.14s/it]\n",
      "260it [00:49,  5.27it/s]\n",
      "4it [00:04,  1.09s/it]\n",
      "260it [00:49,  5.30it/s]\n",
      "4it [00:04,  1.09s/it]\n",
      "260it [00:47,  5.46it/s]\n",
      "4it [00:04,  1.18s/it]\n",
      "260it [00:49,  5.26it/s]\n",
      "4it [00:04,  1.22s/it]\n",
      "260it [00:48,  5.34it/s]\n",
      "4it [00:04,  1.22s/it]\n",
      "260it [00:48,  5.41it/s]\n",
      "4it [00:04,  1.17s/it]\n",
      "260it [00:48,  5.34it/s]\n",
      "4it [00:04,  1.14s/it]\n",
      "260it [00:52,  4.95it/s]\n",
      "4it [00:04,  1.13s/it]\n",
      "260it [00:49,  5.26it/s]\n",
      "4it [00:04,  1.19s/it]\n",
      "260it [00:48,  5.34it/s]\n",
      "4it [00:04,  1.16s/it]\n",
      "260it [00:51,  5.09it/s]\n",
      "4it [00:04,  1.13s/it]\n",
      "260it [00:53,  4.82it/s]\n",
      "4it [00:04,  1.24s/it]\n",
      "260it [00:48,  5.31it/s]\n",
      "4it [00:04,  1.16s/it]\n",
      "260it [00:48,  5.36it/s]\n",
      "4it [00:04,  1.07s/it]\n",
      "260it [00:46,  5.63it/s]\n",
      "4it [00:04,  1.15s/it]\n",
      "46it [00:07,  5.90it/s]"
     ]
    }
   ],
   "source": [
    "patience = 0\n",
    "max_objective = 0\n",
    "losses, corrs, lrs, val_corrs = [], [], [], []\n",
    "for train_loop in range(1000):\n",
    "    current_objective = val_step()\n",
    "    if train_loop > 20:\n",
    "        max_objective = np.max(np.array(val_corrs[:-1])[~np.isnan(val_corrs[:-1])])\n",
    "    if current_objective < max_objective:\n",
    "        patience += 1\n",
    "    else:\n",
    "        patience = 0\n",
    "    if patience >=25:\n",
    "        break\n",
    "    val_corrs.append(current_objective)\n",
    "    for i, (k, b) in tqdm(enumerate(train_dl)):\n",
    "        videos = b[\"screen\"].to(\"cuda\", torch.float32, non_blocking=True).permute(0,2,1,3,4)\n",
    "        responses = b[\"responses\"].to(\"cuda\", torch.float32, non_blocking=True)\n",
    "        out = model(videos, k);\n",
    "        loss = criteria(out.transpose(1,2), responses)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        with warmup_scheduler.dampening():\n",
    "            lr_scheduler.step()\n",
    "    for k, v in train_dl.loaders.items():\n",
    "        v.dataset.shuffle_valid_screen_times()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d8259-4b77-4585-972b-328ffdac2e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127f950-78ed-4cf3-b9ed-2bc463d26268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
