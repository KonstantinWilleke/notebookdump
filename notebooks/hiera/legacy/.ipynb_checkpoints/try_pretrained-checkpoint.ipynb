{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1ff186-46c1-4a8e-bee8-ac2ada76e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199fb3a3-02cd-4963-ba05-32b0e4084dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from experanto.datasets import ChunkDataset, SimpleChunkedDataset\n",
    "from experanto.utils import LongCycler, MultiEpochsDataLoader\n",
    "from experanto.dataloaders import get_multisession_dataloader\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f00a549-4e76-4f73-b2ca-89bcfbc00694",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['dynamic29513-3-5-Video-full',\n",
    "         'dynamic29514-2-9-Video-full',\n",
    "         'dynamic29755-2-8-Video-full',\n",
    "         'dynamic29647-19-8-Video-full',\n",
    "         'dynamic29156-11-10-Video-full',\n",
    "         'dynamic29623-4-9-Video-full',\n",
    "         'dynamic29515-10-12-Video-full',\n",
    "         'dynamic29234-6-9-Video-full',\n",
    "         'dynamic29712-5-9-Video-full',\n",
    "         'dynamic29228-2-10-Video-full'\n",
    "        ]\n",
    "full_paths = [path.join(\"/data/mouse_polly/\", f) for f in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d8eb5d-508a-4ebd-acbc-e68058600ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "cfg.dataset.global_chunk_size = 16\n",
    "cfg.dataset.global_sampling_rate = 8\n",
    "cfg.dataset.modality_config.screen.include_blanks=True\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"train\"}\n",
    "\n",
    "cfg.dataloader.num_workers=4\n",
    "cfg.dataloader.prefetch_factor=1\n",
    "cfg.dataloader.batch_size=16\n",
    "cfg.dataloader.pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420d6fec-9e62-40a8-9afe-1f1b85d7cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the multiple dataloader is an iterator that returns a tuple of (key, batch)\n",
    "train_dl = get_multisession_dataloader(full_paths, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5759d99-761c-4fbf-b70a-4d6b11de820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from hiera import Hiera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91512d0c-adc9-4465-a511-9f36579006f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hiera import Hiera\n",
    "tiny_hiera = Hiera(input_size=(16,224, 224),\n",
    "                     num_heads=4, \n",
    "                     embed_dim=256,\n",
    "                     stages=(2, 6,), \n",
    "                     q_pool=1, \n",
    "                     in_chans=1,\n",
    "                     q_stride=(1, 2, 2),\n",
    "                     mask_unit_size=(1, 8, 8),\n",
    "                     patch_kernel=(3, 8, 8),\n",
    "                     patch_stride=(2, 4, 4),\n",
    "                     patch_padding=(1, 3, 3),\n",
    "                     sep_pos_embed=True,)\n",
    "tiny_hiera = torch.compile(tiny_hiera).cuda().to(torch.bfloat16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16cae4c4-f051-44de-b8ba-838d4d57dcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 28, 28, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2f90d-7c7c-4f1b-859b-428067ef8f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5005b61a-62d3-41ec-8816-837b3e0b5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:168: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m example_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m tiny_hiera(example_in, return_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n\u001b[0;32m----> 3\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m features\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "example_in = torch.ones(8,1,16,224, 224).to(\"cuda\", torch.bfloat16)\n",
    "out = tiny_hiera(example_in, return_intermediates=True);\n",
    "features = out[1][2]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "042f2435-dc9d-4d17-bc5b-1110ff368741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3a77f01-6763-4836-9597-a76e63f1c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tiny_hiera.patch_embed.proj.weight.data.to(torch.float32).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09476e5d-35fc-439f-a50b-3be20b41eb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 3, 8, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "151246e6-35d7-471d-ae3e-7d6c68cedeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Hiera(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(1, 256, kernel_size=(3, 8, 8), stride=(2, 4, 4), padding=(1, 3, 3))\n",
       "    )\n",
       "    (unroll): Unroll()\n",
       "    (reroll): Reroll()\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x HieraBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MaskUnitAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): HieraBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MaskUnitAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (3-7): 5 x HieraBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MaskUnitAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Head(\n",
       "      (dropout): Identity()\n",
       "      (projection): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_hiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ecbbfac-254b-496c-80eb-1b06b7d0e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57615/1952834869.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  some_weights = torch.load(\"mae_hiera_huge_16x224.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['_orig_mod.pos_embed_spatial', '_orig_mod.pos_embed_temporal', '_orig_mod.patch_embed.proj.weight', '_orig_mod.patch_embed.proj.bias', '_orig_mod.blocks.0.norm1.weight', '_orig_mod.blocks.0.norm1.bias', '_orig_mod.blocks.0.attn.qkv.weight', '_orig_mod.blocks.0.attn.qkv.bias', '_orig_mod.blocks.0.attn.proj.weight', '_orig_mod.blocks.0.attn.proj.bias', '_orig_mod.blocks.0.norm2.weight', '_orig_mod.blocks.0.norm2.bias', '_orig_mod.blocks.0.mlp.fc1.weight', '_orig_mod.blocks.0.mlp.fc1.bias', '_orig_mod.blocks.0.mlp.fc2.weight', '_orig_mod.blocks.0.mlp.fc2.bias', '_orig_mod.blocks.1.norm1.weight', '_orig_mod.blocks.1.norm1.bias', '_orig_mod.blocks.1.attn.qkv.weight', '_orig_mod.blocks.1.attn.qkv.bias', '_orig_mod.blocks.1.attn.proj.weight', '_orig_mod.blocks.1.attn.proj.bias', '_orig_mod.blocks.1.norm2.weight', '_orig_mod.blocks.1.norm2.bias', '_orig_mod.blocks.1.mlp.fc1.weight', '_orig_mod.blocks.1.mlp.fc1.bias', '_orig_mod.blocks.1.mlp.fc2.weight', '_orig_mod.blocks.1.mlp.fc2.bias', '_orig_mod.blocks.2.norm1.weight', '_orig_mod.blocks.2.norm1.bias', '_orig_mod.blocks.2.attn.qkv.weight', '_orig_mod.blocks.2.attn.qkv.bias', '_orig_mod.blocks.2.attn.proj.weight', '_orig_mod.blocks.2.attn.proj.bias', '_orig_mod.blocks.2.norm2.weight', '_orig_mod.blocks.2.norm2.bias', '_orig_mod.blocks.2.mlp.fc1.weight', '_orig_mod.blocks.2.mlp.fc1.bias', '_orig_mod.blocks.2.mlp.fc2.weight', '_orig_mod.blocks.2.mlp.fc2.bias', '_orig_mod.blocks.2.proj.weight', '_orig_mod.blocks.2.proj.bias', '_orig_mod.blocks.3.norm1.weight', '_orig_mod.blocks.3.norm1.bias', '_orig_mod.blocks.3.attn.qkv.weight', '_orig_mod.blocks.3.attn.qkv.bias', '_orig_mod.blocks.3.attn.proj.weight', '_orig_mod.blocks.3.attn.proj.bias', '_orig_mod.blocks.3.norm2.weight', '_orig_mod.blocks.3.norm2.bias', '_orig_mod.blocks.3.mlp.fc1.weight', '_orig_mod.blocks.3.mlp.fc1.bias', '_orig_mod.blocks.3.mlp.fc2.weight', '_orig_mod.blocks.3.mlp.fc2.bias', '_orig_mod.blocks.4.norm1.weight', '_orig_mod.blocks.4.norm1.bias', '_orig_mod.blocks.4.attn.qkv.weight', '_orig_mod.blocks.4.attn.qkv.bias', '_orig_mod.blocks.4.attn.proj.weight', '_orig_mod.blocks.4.attn.proj.bias', '_orig_mod.blocks.4.norm2.weight', '_orig_mod.blocks.4.norm2.bias', '_orig_mod.blocks.4.mlp.fc1.weight', '_orig_mod.blocks.4.mlp.fc1.bias', '_orig_mod.blocks.4.mlp.fc2.weight', '_orig_mod.blocks.4.mlp.fc2.bias', '_orig_mod.blocks.5.norm1.weight', '_orig_mod.blocks.5.norm1.bias', '_orig_mod.blocks.5.attn.qkv.weight', '_orig_mod.blocks.5.attn.qkv.bias', '_orig_mod.blocks.5.attn.proj.weight', '_orig_mod.blocks.5.attn.proj.bias', '_orig_mod.blocks.5.norm2.weight', '_orig_mod.blocks.5.norm2.bias', '_orig_mod.blocks.5.mlp.fc1.weight', '_orig_mod.blocks.5.mlp.fc1.bias', '_orig_mod.blocks.5.mlp.fc2.weight', '_orig_mod.blocks.5.mlp.fc2.bias', '_orig_mod.blocks.6.norm1.weight', '_orig_mod.blocks.6.norm1.bias', '_orig_mod.blocks.6.attn.qkv.weight', '_orig_mod.blocks.6.attn.qkv.bias', '_orig_mod.blocks.6.attn.proj.weight', '_orig_mod.blocks.6.attn.proj.bias', '_orig_mod.blocks.6.norm2.weight', '_orig_mod.blocks.6.norm2.bias', '_orig_mod.blocks.6.mlp.fc1.weight', '_orig_mod.blocks.6.mlp.fc1.bias', '_orig_mod.blocks.6.mlp.fc2.weight', '_orig_mod.blocks.6.mlp.fc2.bias', '_orig_mod.blocks.7.norm1.weight', '_orig_mod.blocks.7.norm1.bias', '_orig_mod.blocks.7.attn.qkv.weight', '_orig_mod.blocks.7.attn.qkv.bias', '_orig_mod.blocks.7.attn.proj.weight', '_orig_mod.blocks.7.attn.proj.bias', '_orig_mod.blocks.7.norm2.weight', '_orig_mod.blocks.7.norm2.bias', '_orig_mod.blocks.7.mlp.fc1.weight', '_orig_mod.blocks.7.mlp.fc1.bias', '_orig_mod.blocks.7.mlp.fc2.weight', '_orig_mod.blocks.7.mlp.fc2.bias', '_orig_mod.norm.weight', '_orig_mod.norm.bias', '_orig_mod.head.projection.weight', '_orig_mod.head.projection.bias'], unexpected_keys=['pos_embed_spatial', 'pos_embed_temporal', 'decoder_pos_embed', 'mask_token', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.2.proj.weight', 'blocks.2.proj.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.8.proj.weight', 'blocks.8.proj.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias', 'blocks.12.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.12.mlp.fc1.weight', 'blocks.12.mlp.fc1.bias', 'blocks.12.mlp.fc2.weight', 'blocks.12.mlp.fc2.bias', 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.attn.qkv.weight', 'blocks.13.attn.qkv.bias', 'blocks.13.attn.proj.weight', 'blocks.13.attn.proj.bias', 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.mlp.fc1.bias', 'blocks.13.mlp.fc2.weight', 'blocks.13.mlp.fc2.bias', 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.attn.qkv.weight', 'blocks.14.attn.qkv.bias', 'blocks.14.attn.proj.weight', 'blocks.14.attn.proj.bias', 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.14.mlp.fc1.weight', 'blocks.14.mlp.fc1.bias', 'blocks.14.mlp.fc2.weight', 'blocks.14.mlp.fc2.bias', 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.attn.qkv.weight', 'blocks.15.attn.qkv.bias', 'blocks.15.attn.proj.weight', 'blocks.15.attn.proj.bias', 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.15.mlp.fc1.weight', 'blocks.15.mlp.fc1.bias', 'blocks.15.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.attn.qkv.weight', 'blocks.16.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.16.attn.proj.bias', 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.16.mlp.fc1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.16.mlp.fc2.weight', 'blocks.16.mlp.fc2.bias', 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.qkv.bias', 'blocks.17.attn.proj.weight', 'blocks.17.attn.proj.bias', 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.17.mlp.fc1.weight', 'blocks.17.mlp.fc1.bias', 'blocks.17.mlp.fc2.weight', 'blocks.17.mlp.fc2.bias', 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.attn.qkv.weight', 'blocks.18.attn.qkv.bias', 'blocks.18.attn.proj.weight', 'blocks.18.attn.proj.bias', 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.18.mlp.fc1.weight', 'blocks.18.mlp.fc1.bias', 'blocks.18.mlp.fc2.weight', 'blocks.18.mlp.fc2.bias', 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.attn.qkv.weight', 'blocks.19.attn.qkv.bias', 'blocks.19.attn.proj.weight', 'blocks.19.attn.proj.bias', 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.19.mlp.fc1.weight', 'blocks.19.mlp.fc1.bias', 'blocks.19.mlp.fc2.weight', 'blocks.19.mlp.fc2.bias', 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.attn.qkv.weight', 'blocks.20.attn.qkv.bias', 'blocks.20.attn.proj.weight', 'blocks.20.attn.proj.bias', 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.20.mlp.fc1.weight', 'blocks.20.mlp.fc1.bias', 'blocks.20.mlp.fc2.weight', 'blocks.20.mlp.fc2.bias', 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.attn.qkv.weight', 'blocks.21.attn.qkv.bias', 'blocks.21.attn.proj.weight', 'blocks.21.attn.proj.bias', 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.21.mlp.fc1.weight', 'blocks.21.mlp.fc1.bias', 'blocks.21.mlp.fc2.weight', 'blocks.21.mlp.fc2.bias', 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.attn.qkv.weight', 'blocks.22.attn.qkv.bias', 'blocks.22.attn.proj.weight', 'blocks.22.attn.proj.bias', 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.22.mlp.fc1.weight', 'blocks.22.mlp.fc1.bias', 'blocks.22.mlp.fc2.weight', 'blocks.22.mlp.fc2.bias', 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.attn.qkv.weight', 'blocks.23.attn.qkv.bias', 'blocks.23.attn.proj.weight', 'blocks.23.attn.proj.bias', 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'blocks.23.mlp.fc1.weight', 'blocks.23.mlp.fc1.bias', 'blocks.23.mlp.fc2.weight', 'blocks.23.mlp.fc2.bias', 'blocks.24.norm1.weight', 'blocks.24.norm1.bias', 'blocks.24.attn.qkv.weight', 'blocks.24.attn.qkv.bias', 'blocks.24.attn.proj.weight', 'blocks.24.attn.proj.bias', 'blocks.24.norm2.weight', 'blocks.24.norm2.bias', 'blocks.24.mlp.fc1.weight', 'blocks.24.mlp.fc1.bias', 'blocks.24.mlp.fc2.weight', 'blocks.24.mlp.fc2.bias', 'blocks.25.norm1.weight', 'blocks.25.norm1.bias', 'blocks.25.attn.qkv.weight', 'blocks.25.attn.qkv.bias', 'blocks.25.attn.proj.weight', 'blocks.25.attn.proj.bias', 'blocks.25.norm2.weight', 'blocks.25.norm2.bias', 'blocks.25.mlp.fc1.weight', 'blocks.25.mlp.fc1.bias', 'blocks.25.mlp.fc2.weight', 'blocks.25.mlp.fc2.bias', 'blocks.26.norm1.weight', 'blocks.26.norm1.bias', 'blocks.26.attn.qkv.weight', 'blocks.26.attn.qkv.bias', 'blocks.26.attn.proj.weight', 'blocks.26.attn.proj.bias', 'blocks.26.norm2.weight', 'blocks.26.norm2.bias', 'blocks.26.mlp.fc1.weight', 'blocks.26.mlp.fc1.bias', 'blocks.26.mlp.fc2.weight', 'blocks.26.mlp.fc2.bias', 'blocks.27.norm1.weight', 'blocks.27.norm1.bias', 'blocks.27.attn.qkv.weight', 'blocks.27.attn.qkv.bias', 'blocks.27.attn.proj.weight', 'blocks.27.attn.proj.bias', 'blocks.27.norm2.weight', 'blocks.27.norm2.bias', 'blocks.27.mlp.fc1.weight', 'blocks.27.mlp.fc1.bias', 'blocks.27.mlp.fc2.weight', 'blocks.27.mlp.fc2.bias', 'blocks.28.norm1.weight', 'blocks.28.norm1.bias', 'blocks.28.attn.qkv.weight', 'blocks.28.attn.qkv.bias', 'blocks.28.attn.proj.weight', 'blocks.28.attn.proj.bias', 'blocks.28.norm2.weight', 'blocks.28.norm2.bias', 'blocks.28.mlp.fc1.weight', 'blocks.28.mlp.fc1.bias', 'blocks.28.mlp.fc2.weight', 'blocks.28.mlp.fc2.bias', 'blocks.29.norm1.weight', 'blocks.29.norm1.bias', 'blocks.29.attn.qkv.weight', 'blocks.29.attn.qkv.bias', 'blocks.29.attn.proj.weight', 'blocks.29.attn.proj.bias', 'blocks.29.norm2.weight', 'blocks.29.norm2.bias', 'blocks.29.mlp.fc1.weight', 'blocks.29.mlp.fc1.bias', 'blocks.29.mlp.fc2.weight', 'blocks.29.mlp.fc2.bias', 'blocks.30.norm1.weight', 'blocks.30.norm1.bias', 'blocks.30.attn.qkv.weight', 'blocks.30.attn.qkv.bias', 'blocks.30.attn.proj.weight', 'blocks.30.attn.proj.bias', 'blocks.30.norm2.weight', 'blocks.30.norm2.bias', 'blocks.30.mlp.fc1.weight', 'blocks.30.mlp.fc1.bias', 'blocks.30.mlp.fc2.weight', 'blocks.30.mlp.fc2.bias', 'blocks.31.norm1.weight', 'blocks.31.norm1.bias', 'blocks.31.attn.qkv.weight', 'blocks.31.attn.qkv.bias', 'blocks.31.attn.proj.weight', 'blocks.31.attn.proj.bias', 'blocks.31.norm2.weight', 'blocks.31.norm2.bias', 'blocks.31.mlp.fc1.weight', 'blocks.31.mlp.fc1.bias', 'blocks.31.mlp.fc2.weight', 'blocks.31.mlp.fc2.bias', 'blocks.32.norm1.weight', 'blocks.32.norm1.bias', 'blocks.32.attn.qkv.weight', 'blocks.32.attn.qkv.bias', 'blocks.32.attn.proj.weight', 'blocks.32.attn.proj.bias', 'blocks.32.norm2.weight', 'blocks.32.norm2.bias', 'blocks.32.mlp.fc1.weight', 'blocks.32.mlp.fc1.bias', 'blocks.32.mlp.fc2.weight', 'blocks.32.mlp.fc2.bias', 'blocks.33.norm1.weight', 'blocks.33.norm1.bias', 'blocks.33.attn.qkv.weight', 'blocks.33.attn.qkv.bias', 'blocks.33.attn.proj.weight', 'blocks.33.attn.proj.bias', 'blocks.33.norm2.weight', 'blocks.33.norm2.bias', 'blocks.33.mlp.fc1.weight', 'blocks.33.mlp.fc1.bias', 'blocks.33.mlp.fc2.weight', 'blocks.33.mlp.fc2.bias', 'blocks.34.norm1.weight', 'blocks.34.norm1.bias', 'blocks.34.attn.qkv.weight', 'blocks.34.attn.qkv.bias', 'blocks.34.attn.proj.weight', 'blocks.34.attn.proj.bias', 'blocks.34.norm2.weight', 'blocks.34.norm2.bias', 'blocks.34.mlp.fc1.weight', 'blocks.34.mlp.fc1.bias', 'blocks.34.mlp.fc2.weight', 'blocks.34.mlp.fc2.bias', 'blocks.35.norm1.weight', 'blocks.35.norm1.bias', 'blocks.35.attn.qkv.weight', 'blocks.35.attn.qkv.bias', 'blocks.35.attn.proj.weight', 'blocks.35.attn.proj.bias', 'blocks.35.norm2.weight', 'blocks.35.norm2.bias', 'blocks.35.mlp.fc1.weight', 'blocks.35.mlp.fc1.bias', 'blocks.35.mlp.fc2.weight', 'blocks.35.mlp.fc2.bias', 'blocks.36.norm1.weight', 'blocks.36.norm1.bias', 'blocks.36.attn.qkv.weight', 'blocks.36.attn.qkv.bias', 'blocks.36.attn.proj.weight', 'blocks.36.attn.proj.bias', 'blocks.36.norm2.weight', 'blocks.36.norm2.bias', 'blocks.36.mlp.fc1.weight', 'blocks.36.mlp.fc1.bias', 'blocks.36.mlp.fc2.weight', 'blocks.36.mlp.fc2.bias', 'blocks.37.norm1.weight', 'blocks.37.norm1.bias', 'blocks.37.attn.qkv.weight', 'blocks.37.attn.qkv.bias', 'blocks.37.attn.proj.weight', 'blocks.37.attn.proj.bias', 'blocks.37.norm2.weight', 'blocks.37.norm2.bias', 'blocks.37.mlp.fc1.weight', 'blocks.37.mlp.fc1.bias', 'blocks.37.mlp.fc2.weight', 'blocks.37.mlp.fc2.bias', 'blocks.38.norm1.weight', 'blocks.38.norm1.bias', 'blocks.38.attn.qkv.weight', 'blocks.38.attn.qkv.bias', 'blocks.38.attn.proj.weight', 'blocks.38.attn.proj.bias', 'blocks.38.norm2.weight', 'blocks.38.norm2.bias', 'blocks.38.mlp.fc1.weight', 'blocks.38.mlp.fc1.bias', 'blocks.38.mlp.fc2.weight', 'blocks.38.mlp.fc2.bias', 'blocks.39.norm1.weight', 'blocks.39.norm1.bias', 'blocks.39.attn.qkv.weight', 'blocks.39.attn.qkv.bias', 'blocks.39.attn.proj.weight', 'blocks.39.attn.proj.bias', 'blocks.39.norm2.weight', 'blocks.39.norm2.bias', 'blocks.39.mlp.fc1.weight', 'blocks.39.mlp.fc1.bias', 'blocks.39.mlp.fc2.weight', 'blocks.39.mlp.fc2.bias', 'blocks.40.norm1.weight', 'blocks.40.norm1.bias', 'blocks.40.attn.qkv.weight', 'blocks.40.attn.qkv.bias', 'blocks.40.attn.proj.weight', 'blocks.40.attn.proj.bias', 'blocks.40.norm2.weight', 'blocks.40.norm2.bias', 'blocks.40.mlp.fc1.weight', 'blocks.40.mlp.fc1.bias', 'blocks.40.mlp.fc2.weight', 'blocks.40.mlp.fc2.bias', 'blocks.41.norm1.weight', 'blocks.41.norm1.bias', 'blocks.41.attn.qkv.weight', 'blocks.41.attn.qkv.bias', 'blocks.41.attn.proj.weight', 'blocks.41.attn.proj.bias', 'blocks.41.norm2.weight', 'blocks.41.norm2.bias', 'blocks.41.mlp.fc1.weight', 'blocks.41.mlp.fc1.bias', 'blocks.41.mlp.fc2.weight', 'blocks.41.mlp.fc2.bias', 'blocks.42.norm1.weight', 'blocks.42.norm1.bias', 'blocks.42.attn.qkv.weight', 'blocks.42.attn.qkv.bias', 'blocks.42.attn.proj.weight', 'blocks.42.attn.proj.bias', 'blocks.42.norm2.weight', 'blocks.42.norm2.bias', 'blocks.42.mlp.fc1.weight', 'blocks.42.mlp.fc1.bias', 'blocks.42.mlp.fc2.weight', 'blocks.42.mlp.fc2.bias', 'blocks.43.norm1.weight', 'blocks.43.norm1.bias', 'blocks.43.attn.qkv.weight', 'blocks.43.attn.qkv.bias', 'blocks.43.attn.proj.weight', 'blocks.43.attn.proj.bias', 'blocks.43.norm2.weight', 'blocks.43.norm2.bias', 'blocks.43.mlp.fc1.weight', 'blocks.43.mlp.fc1.bias', 'blocks.43.mlp.fc2.weight', 'blocks.43.mlp.fc2.bias', 'blocks.44.norm1.weight', 'blocks.44.norm1.bias', 'blocks.44.attn.qkv.weight', 'blocks.44.attn.qkv.bias', 'blocks.44.attn.proj.weight', 'blocks.44.attn.proj.bias', 'blocks.44.norm2.weight', 'blocks.44.norm2.bias', 'blocks.44.mlp.fc1.weight', 'blocks.44.mlp.fc1.bias', 'blocks.44.mlp.fc2.weight', 'blocks.44.mlp.fc2.bias', 'blocks.44.proj.weight', 'blocks.44.proj.bias', 'blocks.45.norm1.weight', 'blocks.45.norm1.bias', 'blocks.45.attn.qkv.weight', 'blocks.45.attn.qkv.bias', 'blocks.45.attn.proj.weight', 'blocks.45.attn.proj.bias', 'blocks.45.norm2.weight', 'blocks.45.norm2.bias', 'blocks.45.mlp.fc1.weight', 'blocks.45.mlp.fc1.bias', 'blocks.45.mlp.fc2.weight', 'blocks.45.mlp.fc2.bias', 'blocks.46.norm1.weight', 'blocks.46.norm1.bias', 'blocks.46.attn.qkv.weight', 'blocks.46.attn.qkv.bias', 'blocks.46.attn.proj.weight', 'blocks.46.attn.proj.bias', 'blocks.46.norm2.weight', 'blocks.46.norm2.bias', 'blocks.46.mlp.fc1.weight', 'blocks.46.mlp.fc1.bias', 'blocks.46.mlp.fc2.weight', 'blocks.46.mlp.fc2.bias', 'blocks.47.norm1.weight', 'blocks.47.norm1.bias', 'blocks.47.attn.qkv.weight', 'blocks.47.attn.qkv.bias', 'blocks.47.attn.proj.weight', 'blocks.47.attn.proj.bias', 'blocks.47.norm2.weight', 'blocks.47.norm2.bias', 'blocks.47.mlp.fc1.weight', 'blocks.47.mlp.fc1.bias', 'blocks.47.mlp.fc2.weight', 'blocks.47.mlp.fc2.bias', 'multi_scale_fusion_heads.0.weight', 'multi_scale_fusion_heads.0.bias', 'multi_scale_fusion_heads.1.weight', 'multi_scale_fusion_heads.1.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias', 'encoder_norm.weight', 'encoder_norm.bias', 'decoder_embed.weight', 'decoder_embed.bias'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_weights = torch.load(\"mae_hiera_huge_16x224.pth\")\n",
    "tiny_hiera.load_state_dict(some_weights[\"model_state\"], strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c66ebf-8da4-4790-a568-deb5cd10b544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
