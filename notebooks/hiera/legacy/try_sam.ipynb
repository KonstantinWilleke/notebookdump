{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbb930b-c616-4435-8031-a44e1f2343de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68175db-fcc6-43d8-a452-5c3ac3150575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.modeling.backbones.hieradet import Hiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830c8ece-7f2e-48c0-b088-0138afae0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sam2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fe6b0-93d9-448b-a791-463647ad540a",
   "metadata": {},
   "source": [
    "sam2??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de97c1fc-343e-4938-89fb-d915d99de5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'sam2' from '/usr/local/lib/python3.10/dist-packages/sam2/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        /usr/local/lib/python3.10/dist-packages/sam2/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sam2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef55a74-4f86-4150-85cc-1160b9e4c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "nano_hiera = Hiera(embed_dim=96, num_heads=1, stages=(4, 4, 4), window_spec=(8, 4, 14), q_pool=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fb0581-bd98-40d0-a777-5841b3877af6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9179520\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "n_params=0\n",
    "for n, p in nano_hiera.named_parameters():\n",
    "    #print(n, p.shape, torch.numel(p))\n",
    "    n_params += torch.numel(p)\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "790f87e4-5df6-413e-a67f-4868e3adec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c07468-a781-45b4-8274-4d55bdc1efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [512, 3, 10, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)):\n\u001b[0;32m----> 5\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mnano_hiera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sam2/modeling/backbones/hieradet.py:284\u001b[0m, in \u001b[0;36mHiera.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 284\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# x: (B, H, W, C)\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# Add pos embed\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_embed(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sam2/modeling/backbones/utils.py:92\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# B C H W -> B H W C\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [512, 3, 10, 224, 224]"
     ]
    }
   ],
   "source": [
    "my_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(512,3,10, 224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87a2a98-3687-453c-96ce-b2e39f9419cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cb1297-ccee-4b64-956b-f159a7ee675f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6207bcd3-2fb9-4a1a-9213-e3a28f5671f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:10<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "my_hiera = my_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(500,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(100)):\n",
    "            out = my_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4eea2ee-6c73-41b5-941c-f8ab22775275",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hiera = torch.compile(my_hiera, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e032d6a4-4650-4ba6-b561-f09576b27acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hiera = my_hiera.to(\"cuda\", torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88839aa-8921-42bb-816c-077f15ab4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.ones(512,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "out = my_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db305b0b-ae92-4483-9a92-e0eb79d6dd69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d5e46f-6a7b-4c7b-a4a8-21fe772e6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:14<00:00,  7.13it/s]\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(128*4,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = my_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb95d35a-f0ad-4a2c-a677-b71088b15874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:08<00:00, 11.97it/s]\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(128*4,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = my_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c130f4-90b3-4660-bf9b-946698df7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:08<00:00, 11.97it/s]\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(128*4,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = my_hiera(example, );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf98bb-fe60-4f52-94f5-9f500d5618ba",
   "metadata": {},
   "source": [
    "# try nano hiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7304ea9f-72ae-405e-9ce9-1d25bf771404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35992452-a774-460c-9c09-9907312a7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3157c7-0dcc-45ca-86fa-766481717f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:14<00:00,  6.93it/s]\n"
     ]
    }
   ],
   "source": [
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(128*8,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0977b60-92bb-4eab-8e2f-f7fddc2fd580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:14<00:00,  7.06it/s]\n"
     ]
    }
   ],
   "source": [
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(128*8,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121773e9-1ca2-4592-bd51-b595389d25a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7168"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f8a5a3-6b61-42d5-8656-8d62e86193a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:07<00:00, 14.18it/s]\n"
     ]
    }
   ],
   "source": [
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(512,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(100)):\n",
    "            out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e2e0ec1-22a3-41fa-92cd-a797227f9c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:06<00:00, 14.55it/s]\n"
     ]
    }
   ],
   "source": [
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(500,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc72e5e-ca22-41bf-a1d1-2af88d603805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "nano_hiera = torch.compile(nano_hiera, )\n",
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(128*8,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432e15a9-f620-4c82-a067-b46585685693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:10<00:00,  9.90it/s]\n"
     ]
    }
   ],
   "source": [
    "nano_hiera = nano_hiera.to(\"cuda\", torch.bfloat16)\n",
    "example = torch.ones(128*8,3,224,224).to(\"cuda\", torch.bfloat16)\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(100)):\n",
    "            out = nano_hiera(example, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e1b89-83fb-4de5-8cc6-64dfc92b2a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c0e177-765d-4d2d-862a-d1c19d62674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.5*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca1a7a8e-f0c6-44bb-9d73-c55c91c3c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1152 / (16*2*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09dd44d0-6f5c-46b7-845f-ba8ea54459ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "\n",
    "predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaaf967-ad49-4097-a18c-4787e99f3594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
